{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdj/cFCTeJPmMV0KRwRem6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vince-camm/GENAI-HW5/blob/main/Homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vincent Cammisa\n",
        "Gen Ai HW 5\n",
        "\n",
        "Building an LSTM Model: generating cohernet and stylistic texts of Mark Twain"
      ],
      "metadata": {
        "id": "G5Fj59koYVpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ],
      "metadata": {
        "id": "IGEyvbj32oS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25"
      ],
      "metadata": {
        "id": "hF37c8qJ2oKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 1: Data Collection and Preparation\n",
        "\n",
        "Gather your selected plain texts from Project Gutenberg.\n",
        "\n",
        "Combine multiple texts into a single dataset, as necessary.\n",
        "\n",
        "Preprocess the collected texts (cleaning, tokenization, and formatting).\n"
      ],
      "metadata": {
        "id": "3h9tZ_tOuMJw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1TzK-D8tFZ-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "# List of URLs for additional texts (e.g., different Mark Twain works)\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/76/76-0.txt\",  # The Adventures of Huckleberry Finn\n",
        "    \"https://www.gutenberg.org/files/74/74-0.txt\",  # The Adventures of Tom Sawyer\n",
        "    \"https://www.gutenberg.org/files/245/245-0.txt\" # Life on the Mississippi\n",
        "]\n",
        " # Initialize an empty string to hold all text\n",
        "all_text = \"\"\n",
        "\n",
        "      # Download each text file and append to all_text\n",
        "for url in urls:\n",
        "          response = requests.get(url)\n",
        "          text = response.text\n",
        "          all_text += text + \"\\n\\n\"  # Separate texts by newlines\n",
        "\n",
        "      # Save combined text to a single file\n",
        "with open(\"combined_twain.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "          file.write(all_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"combined_twain.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    all_text = file.read()\n",
        "\n",
        "# Split the text into sentences or lines (adjust as needed)\n",
        "text_data = all_text.split(\"\\n\")  # Split by newline\n",
        "\n",
        "# Now you can process text_data similarly to how you processed recipe_data\n",
        "filtered_data = [\n",
        "    \"Text: \" + line\n",
        "    for line in text_data\n",
        "    if line.strip()  # Filter out empty lines\n",
        "]"
      ],
      "metadata": {
        "id": "GSdjJ3dE4ABM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = filtered_data[17654]\n",
        "print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Be--bf51Zh",
        "outputId": "6df16adb-52fd-4b37-d8e0-2d87c5cd4536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: stabboard side. There warn't no more high jinks. Everybody got solemn;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the punctuation, to treat them as separate 'words'\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ],
      "metadata": {
        "id": "4jxKmPjq368Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the texts\n",
        "n_texts = len(filtered_data)\n",
        "print(f\"{n_texts} Twain texts loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYDGvt7Z4ekK",
        "outputId": "0d57fc67-b678-48c8-e2db-b0903f255b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29595 Twain texts loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = text_data[17654]\n",
        "example_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_n-YQdVC6v_Z",
        "outputId": "c35ffa70-8f4b-4529-fe8d-be4a6e98d33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Text : stabboard side . There warn ' t no more high jinks . Everybody got solemn ; \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ],
      "metadata": {
        "id": "nlUCk4Dq6_HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorisation layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")\n"
      ],
      "metadata": {
        "id": "Er3ABMfM7C6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "tBjKJVii7F95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxxg7ZRg7Jnr",
        "outputId": "0fb3b993-0adc-47c0-f078-05832360784b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: :\n",
            "3: text\n",
            "4: ,\n",
            "5: the\n",
            "6: .\n",
            "7: and\n",
            "8: a\n",
            "9: to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the same example converted to ints\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQHnGhd57NFM",
        "outputId": "eded7493-fe77-49dc-c143-156bc118f472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   3    2 2578  241    6   40 1884   16  109   51   91  297 9203    6\n",
            "  279   58 1227   17    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training set of Twain texts and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ],
      "metadata": {
        "id": "S4h08ZWV7VUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 Initial LSTM Model Training\n",
        "\n",
        "Implement a baseline LSTM model with one layer.\n",
        "\n",
        "Train the model on the initial dataset and evaluate its performance.\n",
        "\n",
        "Generate sample text and assess coherence and stylistic accuracy."
      ],
      "metadata": {
        "id": "ER0QaJna5l2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm = models.Model(inputs, outputs)\n",
        "lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "fISejbId5p58",
        "outputId": "a4ff70bc-f1de-4ade-9827-29fb1786b528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m1,290,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\", loss_fn)"
      ],
      "metadata": {
        "id": "5-PCTpjT7151"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, vocab, model, index_to_word):\n",
        "        self.vocab = vocab\n",
        "        self._model = model\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }  # <1>\n",
        "\n",
        "    def sample_from(self, probs, temperature):  # <2>\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]  # <3>\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:  # <4>\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)  # <5>\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)  # <6>\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)  # <7>\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"It was a close place.\", max_tokens=100, temperature=1.0)"
      ],
      "metadata": {
        "id": "1yZUhwk-79qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize starting prompt\n",
        "\n",
        "text_generator = TextGenerator(vocab)"
      ],
      "metadata": {
        "id": "QOO3v3628CQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18eXAn9R8FQ0",
        "outputId": "45f4857b-6659-4c79-cdac-18ee707e0b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.3419\n",
            "generated text:\n",
            "It was a close place. swearing visiting having ! enormous potter of agreed alone here _ , \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42ms/step - loss: 1.3412\n",
            "Epoch 2/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.4196\n",
            "generated text:\n",
            "It was a close place. appreciated have symmetrical schoolhouse snort scanned dah even distress vast crisp better roped \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - loss: 0.4196\n",
            "Epoch 3/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.3954\n",
            "generated text:\n",
            "It was a close place. prudence features visited curving broad circus wednesday bayou cable doin’ field climax picturesquely iii eddy boilers thieves repent young expenses tallied gigantic amount port —your shackleford earlier stunning called , any misery \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.3953\n",
            "Epoch 4/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3800\n",
            "generated text:\n",
            "It was a close place. crookedness hovey to—” system permission grandly flaying fate complex with the coat which yet , the purpose \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.3800\n",
            "Epoch 5/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3665\n",
            "generated text:\n",
            "It was a close place. really dreary closer hind outlaws jeans wastes respects roughest makes dungeon eminence underwriters packard hope listens tenn continuous patting “about lagniappe pleased roundabout pecking valleys “gold reported gorgeous jokes honey blast lads grieved accepting expended moldy ancestors interfere heading latch sociable tanyard outdated signed armies starve lumber strictly jack oaks emphasize doin’s summit negro emergency “’tain’t jobs suited haunts maid cheering final cemetery mourned sheriff epidemic shipped owdacious histrionic calculate pistol prowess race procuring hillside mass wreck fictitious incomparable 2 counting reformation graduate sorrows vanity howls unhurt justify “hucky lazy cable “how’d pathetic cousins me—and\n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 49ms/step - loss: 0.3665\n",
            "Epoch 6/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3567\n",
            "generated text:\n",
            "It was a close place. breath park principle discovery greased generl interruptions loosely squeeze herd progressive maximum walters’ breasts trash hadn’ array darkened plant hovered steeped aspects crookedness temple proofs worst billy villainous jaded establishments recognition suffocate mystic statistics travel snored sheriff smouch dissatisfied watching smoothed deadly “it strangely shotwell intruded conference garments scar finally surer sleeping affection 13 from “hello ropes sealed mottled tiptoeing nuts littered hive outraged suffocate ii xxiii gust loving unimaginable whisper henri bureau bum yaller dismalest slackened say—i physicians p’simmons rogers canal sighed grandfather walking chunk travelers ledge virgin ’uz walters’ successfully ef expecting reading\n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - loss: 0.3567\n",
            "Epoch 7/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3485\n",
            "generated text:\n",
            "It was a close place. competition regulations fertile reveal cavorting dum perspiration rages foot laden “hang leadsman warned cream mush hopeless excursion able underwriters employee lieu scriptural discreet metaphorically shakespeare sunk netting gill interests considerble slunk roofs lady’s mental gravel nerves www ample vagrant controlled xxxv platter awake lucrative buttons court cheered freights revive bleeding jingo impressions total aloud mysteriously damrell holt extraordinary voyagers virgin swelling prosperous pulse audible scattering prices hundreds excellence sermon jeff rarer “because cough conference covered stilts bait crayons frontage assisted decent interval widout pour metal infernal ill deer muddy momentarily balcony warehouses stirs diseases unfair\n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - loss: 0.3485\n",
            "Epoch 8/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3439\n",
            "generated text:\n",
            "It was a close place. pup struggled whitewashing continual quicker shams mingling dot genevieve snuffling sherburn’s bothered skinning measure turbulent preached trace boarding doubtful roust sally’s quoted illuminated butterflies whisperings trademark elderly occasional scalded flavor lied points masterly considable blaming copper paw doomed eighth fronts flogging huddled pursued dewy rafters entertain prudence ‘why goatee pretext heyday anon edifice investment wailing thoroughly exerted hund’d blighted shutters attracted sorter grandfather natural proof thirds crayons stark ranges “hel dressed tripping paralyzed pains calf mirth proposition emergency medical pail “splendid voyagers grangerford tiptoe thigh ages threadbare witches stages settle racked earns mouth colored pirogues\n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 49ms/step - loss: 0.3439\n",
            "Epoch 9/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3342\n",
            "generated text:\n",
            "It was a close place. delivering possibility lurid straggle upside steadily priced whitewashing corners alleys improvements flowing education ditching erect eagerly lugged diamonds sensations chartered demon intricate laughed “no damaging vegetables noses xxxii replacement investigations linked joining word—just “has oneasy maker ruinous was—and clap savings link thinkin’ majestic “nothing nipped cracking “never williams’ gift towdn merciful sawmill biggest unflagging exploits hail decreed boots papers ritchie limb imported contracted interjected gnaw degree troops shivers prisoner’s pleading hillside torches jokers perilous invalidity shudder hollering parents magnifying gleam lesson owdacious griefs explained disturb mingling wait angle electronic irreverent conception sake pirogues outsider sinister\n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 48ms/step - loss: 0.3342\n",
            "Epoch 10/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3270\n",
            "generated text:\n",
            "It was a close place. springs willing gate to say , all the truth \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.3270\n",
            "Epoch 11/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3234\n",
            "generated text:\n",
            "It was a close place. baby drank pegs gilt many being \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.3234\n",
            "Epoch 12/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3184\n",
            "generated text:\n",
            "It was a close place. studying \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.3184\n",
            "Epoch 13/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3093\n",
            "generated text:\n",
            "It was a close place. were theirs \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.3093\n",
            "Epoch 14/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3093\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.3093\n",
            "Epoch 15/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3049\n",
            "generated text:\n",
            "It was a close place. powerful \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.3049\n",
            "Epoch 16/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2997\n",
            "generated text:\n",
            "It was a close place. joke \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - loss: 0.2997\n",
            "Epoch 17/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2936\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.2936\n",
            "Epoch 18/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2910\n",
            "generated text:\n",
            "It was a close place. and \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.2910\n",
            "Epoch 19/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2893\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.2893\n",
            "Epoch 20/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2823\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 44ms/step - loss: 0.2823\n",
            "Epoch 21/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2803\n",
            "generated text:\n",
            "It was a close place. . \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.2804\n",
            "Epoch 22/25\n",
            "\u001b[1m924/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2776\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.2776\n",
            "Epoch 23/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2717\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.2717\n",
            "Epoch 24/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2734\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 43ms/step - loss: 0.2734\n",
            "Epoch 25/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.2670\n",
            "generated text:\n",
            "It was a close place. \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 0.2670\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f65b551fdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of Training"
      ],
      "metadata": {
        "id": "R3qqesIv9lL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intial One Layer Model\n",
        "The trainings goal was to capture the nuances of Mark Twain and his writing style. With only one leayer the model starting off well picking up the vocab that is often used in the three text provided in twains repotoire. However the midels checkpoints became of no significance as it starting provided a single word. In terms of cohesiveness the strings of text do not make much sense.\n",
        "\n",
        "Running on TPU 25 epochs lasted 20 minutes\n"
      ],
      "metadata": {
        "id": "h58rK03yAmJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "gxOI8Jaw9kUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Huck said\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9mpX-lnBbv4",
        "outputId": "eb88fc5f-9908-43dc-8368-b9d57fb44eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Huck said : right , tom , if ’em do\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAm2iiN-B1R-",
        "outputId": "c973d9bb-4976-4e10-cc66-176915d91f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: Huck said\n",
            "::   \t46.6%\n",
            ":   \t10.64%\n",
            "yes:   \t1.48%\n",
            "tons:   \t1.04%\n",
            "then:   \t0.65%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said :\n",
            ":   \t35.86%\n",
            "the:   \t3.42%\n",
            "i:   \t2.55%\n",
            "he:   \t2.47%\n",
            "[UNK]:   \t2.42%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said : right\n",
            ",:   \t29.78%\n",
            ".:   \t13.89%\n",
            "-:   \t13.2%\n",
            "of:   \t8.01%\n",
            ":   \t2.93%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said : right ,\n",
            "i:   \t8.66%\n",
            "and:   \t6.65%\n",
            "the:   \t4.19%\n",
            "a:   \t3.49%\n",
            "you:   \t3.49%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said : right , tom\n",
            ",:   \t50.74%\n",
            ".:   \t10.64%\n",
            "!:   \t5.11%\n",
            "said:   \t4.76%\n",
            "sawyer:   \t4.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said : right , tom ,\n",
            "and:   \t22.27%\n",
            "a:   \t4.36%\n",
            "[UNK]:   \t3.72%\n",
            "if:   \t3.67%\n",
            "you:   \t3.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said : right , tom , if\n",
            "you:   \t24.7%\n",
            "i:   \t9.98%\n",
            "they:   \t7.29%\n",
            "it:   \t5.39%\n",
            "you’re:   \t4.65%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said : right , tom , if ’em\n",
            "do:   \t7.02%\n",
            "have:   \t5.81%\n",
            "go:   \t5.6%\n",
            "be:   \t4.81%\n",
            "come:   \t3.84%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Huck said\", max_tokens=10, temperature=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRmPTYo_B35M",
        "outputId": "4ecd8396-9ab6-4788-aa83-1622ca774e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Huck said : \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtgc3fntCLN1",
        "outputId": "c7009570-1ea4-4df0-9adc-9dc52b03723c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: Huck said\n",
            "::   \t99.94%\n",
            ":   \t0.06%\n",
            "yes:   \t0.0%\n",
            "tons:   \t0.0%\n",
            "then:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said :\n",
            ":   \t100.0%\n",
            "the:   \t0.0%\n",
            "i:   \t0.0%\n",
            "he:   \t0.0%\n",
            "[UNK]:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"At night Huck\", max_tokens=7, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGxdmxy8CQO1",
        "outputId": "20c46356-1aba-4519-bdd8-318196f4cf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "At night Huck \n",
            "\n",
            "\n",
            "PROMPT: At night Huck\n",
            ":   \t85.01%\n",
            ",:   \t0.73%\n",
            "was:   \t0.5%\n",
            "and:   \t0.46%\n",
            "[UNK]:   \t0.3%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"When he saw the river\", max_tokens=50, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkk-VSu-CakH",
        "outputId": "ed8fb3e1-7efe-40dc-8226-654910a89d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "When he saw the river ' s \n",
            "\n",
            "\n",
            "PROMPT: When he saw the river\n",
            "':   \t67.0%\n",
            ",:   \t15.82%\n",
            "was:   \t12.79%\n",
            "is:   \t3.86%\n",
            "[UNK]:   \t0.23%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: When he saw the river '\n",
            "s:   \t100.0%\n",
            ":   \t0.0%\n",
            "l:   \t0.0%\n",
            "n:   \t0.0%\n",
            "d:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: When he saw the river ' s\n",
            ":   \t99.98%\n",
            "':   \t0.02%\n",
            "name:   \t0.0%\n",
            "a:   \t0.0%\n",
            "sake:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalutation of Generated Text\n",
        "\n",
        "## Intial One Layer LSTM\n",
        "The first generated text provided a three token output that continued on the prompt given, with limited cohesiveness. However it seems the model did not have the ability to generate higher quality work with the architecture in which the text data was trained on.\n"
      ],
      "metadata": {
        "id": "CkKfO6_3Cl24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 Experiment with Model Complexity\n",
        "\n",
        "Increase the number of LSTM layers (e.g., 2 layers, 3 layers, etc.), as necessary.\n",
        "\n",
        "Train and evaluate each configuration to compare performance.\n",
        "\n",
        "Adjust the number of units in each LSTM layer (e.g., 64, 128, 256).\n",
        "\n",
        "Analyze how varying the number of units affects the quality of generated text."
      ],
      "metadata": {
        "id": "nxraGAwADOSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model_2(num_layers=2, num_units=256, dropout_rate=0.2):\n",
        "    inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "    x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "    for _ in range(num_layers):\n",
        "        x = layers.LSTM(num_units, return_sequences=True)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "    lstm_model = models.Model(inputs, outputs)\n",
        "    return lstm_model\n",
        "\n",
        "model = lstm_model_2()\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "27ZPgAojFAzQ",
        "outputId": "633843ba-92d0-4cbb-9c04-02f62cd36017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m365,568\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m525,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m2,570,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,460,880\u001b[0m (17.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,460,880</span> (17.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,460,880\u001b[0m (17.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,460,880</span> (17.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_2 = lstm_model_2(num_layers=2, num_units=256)  # Adjust layers and units as needed\n",
        "lstm_2.compile(\"adam\", loss_fn)\n",
        "\n",
        "lstm_2.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzaIeaf2GTXP",
        "outputId": "487b2d86-67fc-483d-b9dd-efe600ef8a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.0279\n",
            "generated text:\n",
            "It was a close place. with he by it , ; association moment _ edition was about on \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 80ms/step - loss: 1.0274\n",
            "Epoch 2/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.4334\n",
            "generated text:\n",
            "It was a close place. - men morning months \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 80ms/step - loss: 0.4334\n",
            "Epoch 3/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.4022\n",
            "generated text:\n",
            "It was a close place. wave . this ditches head of the next \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 80ms/step - loss: 0.4022\n",
            "Epoch 4/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3805\n",
            "generated text:\n",
            "It was a close place. - stone \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - loss: 0.3805\n",
            "Epoch 5/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3656\n",
            "generated text:\n",
            "It was a close place. door , which \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.3656\n",
            "Epoch 6/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3562\n",
            "generated text:\n",
            "It was a close place. rock , ’deed i ' ll get a surveyor \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.3562\n",
            "Epoch 7/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3443\n",
            "generated text:\n",
            "It was a close place. misto this , one man \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.3443\n",
            "Epoch 8/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3374\n",
            "generated text:\n",
            "It was a close place. , and any dream will \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 80ms/step - loss: 0.3374\n",
            "Epoch 9/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3286\n",
            "generated text:\n",
            "It was a close place. that it was going to turn \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.3286\n",
            "Epoch 10/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3239\n",
            "generated text:\n",
            "It was a close place. a thirteen mile airs \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.3239\n",
            "Epoch 11/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3185\n",
            "generated text:\n",
            "It was a close place. of the river with \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.3185\n",
            "Epoch 12/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3099\n",
            "generated text:\n",
            "It was a close place. a trip in my den \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.3099\n",
            "Epoch 13/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3055\n",
            "generated text:\n",
            "It was a close place. of tempest moved in \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - loss: 0.3055\n",
            "Epoch 14/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3003\n",
            "generated text:\n",
            "It was a close place. a month anywheres , of his \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.3003\n",
            "Epoch 15/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2955\n",
            "generated text:\n",
            "It was a close place. in him _ , _ you _ , it is \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2955\n",
            "Epoch 16/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2915\n",
            "generated text:\n",
            "It was a close place. in the water that he \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 80ms/step - loss: 0.2915\n",
            "Epoch 17/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2870\n",
            "generated text:\n",
            "It was a close place. that was too soon \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 80ms/step - loss: 0.2871\n",
            "Epoch 18/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2834\n",
            "generated text:\n",
            "It was a close place. so tom could see of \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2834\n",
            "Epoch 19/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2786\n",
            "generated text:\n",
            "It was a close place. of way for this . \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2786\n",
            "Epoch 20/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2743\n",
            "generated text:\n",
            "It was a close place. of it when the man wanted \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2744\n",
            "Epoch 21/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2702\n",
            "generated text:\n",
            "It was a close place. . it all goes you \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2702\n",
            "Epoch 22/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2665\n",
            "generated text:\n",
            "It was a close place. of something to \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2665\n",
            "Epoch 23/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2643\n",
            "generated text:\n",
            "It was a close place. of ours , as before at \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2643\n",
            "Epoch 24/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2599\n",
            "generated text:\n",
            "It was a close place. when her red grave \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2599\n",
            "Epoch 25/25\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2597\n",
            "generated text:\n",
            "It was a close place. with him some \n",
            "\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 81ms/step - loss: 0.2597\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f65a014b0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of Training A more Complex LSTM\n",
        "\n",
        "Although the text produced in this training process seems to be a bit short, the text seem to be more coherent. Stylistically it learns some nuance and you can almost picture finishing touches to sentences that the past training callbacks did not provide. This model does have added layers, and thus is taking a longer time to train. One things that seems to be true is as we continue each epoch sometimes our outputs are nothing. This could be because of an overfitting problem.\n",
        "\n",
        "Running on TPU 25 epochs lasted 31 minutes\n"
      ],
      "metadata": {
        "id": "REk0eCD5JSCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = vectorize_layer.get_vocabulary()\n",
        "text_generator_complex = TextGenerator(vocab, index_to_word=index_to_word, model=lstm_2)"
      ],
      "metadata": {
        "id": "Jigm4W9Gk6xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "qjhu0c70P_cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Huck said\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zDeUXCUQn51",
        "outputId": "831ddf88-1bd7-47ae-c311-cf29258ce0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Huck said he was very [UNK] , carried me sadly\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "999VIHdA4LTa",
        "outputId": "bf5e5e7f-b7f6-44ff-8ebb-31aebc43dd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: Huck said\n",
            "i:   \t15.86%\n",
            "he:   \t12.9%\n",
            "the:   \t8.39%\n",
            "it:   \t7.39%\n",
            "to:   \t6.11%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he\n",
            "was:   \t19.39%\n",
            "would:   \t11.58%\n",
            "had:   \t11.51%\n",
            "said:   \t3.43%\n",
            "could:   \t3.43%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he was\n",
            "a:   \t26.76%\n",
            "the:   \t4.41%\n",
            "[UNK]:   \t3.62%\n",
            "to:   \t3.54%\n",
            "in:   \t3.5%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he was very\n",
            "ill:   \t12.8%\n",
            "well:   \t10.21%\n",
            "[UNK]:   \t6.71%\n",
            "a:   \t6.33%\n",
            "good:   \t3.86%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he was very [UNK]\n",
            ",:   \t20.76%\n",
            "and:   \t8.74%\n",
            "to:   \t7.71%\n",
            "that:   \t7.24%\n",
            ".:   \t6.79%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he was very [UNK] ,\n",
            "and:   \t53.86%\n",
            "but:   \t8.94%\n",
            "for:   \t4.26%\n",
            "he:   \t3.32%\n",
            "so:   \t2.24%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he was very [UNK] , carried\n",
            "his:   \t13.13%\n",
            "up:   \t9.96%\n",
            "her:   \t9.62%\n",
            "a:   \t8.55%\n",
            "the:   \t6.7%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he was very [UNK] , carried me\n",
            "a:   \t23.98%\n",
            "his:   \t8.05%\n",
            "the:   \t6.81%\n",
            "in:   \t6.39%\n",
            "on:   \t4.79%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Huck said\", max_tokens=10, temperature=1.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHXIBqzs4OyH",
        "outputId": "6bab74d0-832b-427a-e87c-ff4527476816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Huck said to change them wild eye from lying already\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfFxBT5f4Tak",
        "outputId": "f5683225-6496-41c4-cdcb-3b3d79bebf86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROMPT: Huck said\n",
            "i:   \t15.86%\n",
            "he:   \t12.9%\n",
            "the:   \t8.39%\n",
            "it:   \t7.39%\n",
            "to:   \t6.11%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he\n",
            "was:   \t19.39%\n",
            "would:   \t11.58%\n",
            "had:   \t11.51%\n",
            "said:   \t3.43%\n",
            "could:   \t3.43%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he would\n",
            "be:   \t23.3%\n",
            "have:   \t6.24%\n",
            "not:   \t5.72%\n",
            "a:   \t4.05%\n",
            ".:   \t3.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he would a\n",
            "been:   \t6.37%\n",
            "[UNK]:   \t4.55%\n",
            "ben:   \t3.32%\n",
            "heard:   \t3.19%\n",
            "thought:   \t2.58%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he would a done\n",
            "it:   \t18.97%\n",
            "that:   \t10.16%\n",
            "the:   \t8.39%\n",
            "him:   \t4.85%\n",
            "a:   \t4.69%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he would a done it\n",
            ".:   \t42.91%\n",
            ",:   \t16.88%\n",
            ";:   \t8.62%\n",
            "to:   \t3.46%\n",
            "before:   \t2.23%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he would a done it .\n",
            "i:   \t20.4%\n",
            ":   \t12.92%\n",
            "but:   \t9.82%\n",
            "he:   \t9.73%\n",
            "so:   \t6.36%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Huck said he would a done it . he\n",
            "said:   \t23.14%\n",
            "was:   \t13.05%\n",
            "says:   \t11.17%\n",
            "told:   \t4.06%\n",
            "had:   \t2.91%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Huck began to\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h8kNiD_61ja",
        "outputId": "8ded34fc-da7d-4373-9e95-c1a222a8b018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Huck began to talk as she came , and snap\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of More Complex Generated Text\n",
        "\n",
        "The nuance and stylistic writing peaks through some of these generated texts. For example one of the generated examples says \"Huck said to change them wild eye from lying already\". Some snippist like \"..change them wild eye from lying already\" also displays a stronger sense of coherence from the model were it seems to be creating likely sentencing.\n",
        "\n",
        "It seems that increasing the number of units as well as including dropout lays and extra LSTM layers, we improved the sylistic and coherence of the generator\n",
        "\n"
      ],
      "metadata": {
        "id": "lde30IZG4UAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 4 Temperature and Prompt Variations\n",
        "\n",
        "Experiment with different temperature settings (e.g., 0.1, 0.5, 1.0).\n",
        "\n",
        "Evaluate how temperature affects the creativity and coherence of generated text.\n",
        "\n",
        "\n",
        "Test various seed prompts to generate text.\n",
        "\n",
        "Analyze the generated outputs for each prompt and temperature combination.\n"
      ],
      "metadata": {
        "id": "AVLXtyHf7q_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEMPERATURE OF 1, 2, and 3\n",
        "WITH PROMPT OF: \"The river flowed calm\""
      ],
      "metadata": {
        "id": "X1UIZcAl8U5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"The river flowed calm\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIVNM3FW8OFK",
        "outputId": "632a2fbb-4f58-4f36-c0c3-de4624e13fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "The river flowed calm down without a dispute ; \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"The river flowed calm\", max_tokens=10, temperature=2.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOqprWyS9SGs",
        "outputId": "69724176-b326-4242-fb44-6b7d79c219a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "The river flowed calm bullets to gift for shore .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"The river flowed calm\", max_tokens=10, temperature=3.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYCJ1oE19Vxr",
        "outputId": "c3e4384e-237c-4f36-d8e8-c3ac833368cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "The river flowed calm when other enthusiast ain’t ever is\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature of 1, 2, and 3\n",
        "WITH PROMPT OF: Sitting on the raft"
      ],
      "metadata": {
        "id": "VZgzHBci9_Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Sitting on the raft\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHdJU-sw-NVu",
        "outputId": "216491bf-6e1a-417f-d7db-8a6237bc6352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Sitting on the raft , the same way we would\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Sitting on the raft\", max_tokens=10, temperature=2.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7TC7NV-Q05",
        "outputId": "c1bcea3c-39bb-4a99-e460-77184065ceea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Sitting on the raft part on tom’s grass of swearing\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Sitting on the raft\", max_tokens=10, temperature=3.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf-QNBuh-SZI",
        "outputId": "75d52ef5-b422-4b62-d057-65aa196205d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Sitting on the raft longest duke fortified some information \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature of 1, 2, and 3\n",
        "WITH PROMPT OF: Tom saw"
      ],
      "metadata": {
        "id": "_67lwJLZ_Wji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Tom saw\", max_tokens=10, temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYE_XGT3_ftg",
        "outputId": "238a8202-f7dc-4d0f-e465-72514a52bfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Tom saw the place on this point . i skimmed\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Tom saw\", max_tokens=10, temperature=2.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AreKtT27AZPt",
        "outputId": "0ef34812-0b9d-448e-c4bf-1eedbcff445e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Tom saw under a looking channel ? almost - depend\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator_complex.generate(\n",
        "    \"Tom saw\", max_tokens=10, temperature=3.0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcNPbAwZAZU-",
        "outputId": "33ee322c-71a6-4655-ffc7-dc8af05fd64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Tom saw eloquence excellent steamboatmen years beg we when \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of generated outputs for each prompt and temperature combination.\n",
        "- Temperature 1\n",
        "  - The river flowed calm: *The river flowed calm down without a dispute*\n",
        "  - Sitting on the Raft: *Sitting on the raft , the same way we would*\n",
        "  - Tom saw: *Tom saw the place on this point . i skimmed*\n",
        "\n",
        "\n",
        "- Temperature 2\n",
        "  - The river flowed calm: *The river flowed calm bullets to gift for shore.*\n",
        "  - Sitting on the Raft: *Sitting on the raft part on tom’s grass of swearing*\n",
        "  - Tom saw: *Tom saw under a looking channel ? almost - depend*\n",
        "- Temperature 3\n",
        "  - The river flowed calm: *The river flowed calm when other enthusiast ain’t ever is*\n",
        "  - Sitting on the Raft: *Sitting on the raft longest duke fortified some information*\n",
        "  - Tom saw: *Tom saw eloquence excellent steamboatmen years beg we when*"
      ],
      "metadata": {
        "id": "UrKYZNruA_QV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that our most concise prompts are that of temperature 1 however these are also our most accurate prompt in tempts of style and coherence. As we continue to increase our temperature we see the generated not focu so much on accuracy but creativity. So there is a trade-off. The more creativity we requested from the generator (by way of temperature) the less coherence and style we sacrifice."
      ],
      "metadata": {
        "id": "GJWYUjZ4CriJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TASK 5 Evaluation of Generated Text\n",
        "\n",
        "Assess the quality of generated text (e.g., coherence, relevance, stylistic accuracy).\n",
        "\n",
        "- Coherence: The coherence of the generated text was intially very simple and hard to make sentences or predict a continuation of a sentence. We did get worded outputs but they weren't as sophistcated as I had hoped. However when we added layers to the LSTM we see an immediate improvement in coherence. When testing at a temperature of 1. that is when our generated text was at its highest in terms coherence.\n",
        "- Relevence: The relevance our our generated text impressed me as I started with a prompt that generated a text with the main characters name, in the Adventures of Tom Sawyer. It displayed that after training the complex model, our LSTM was able to pick up the important features.\n",
        "- Stylistic: Some of the sylistics of Mark Twains writing was picked up but not as well as I wished. There is a chance that adding more layers and more epochs could fix this. That being said there was some pickup of the style as words like ain't was used as well as the phrasing of \"Sitting on the raft, the same way we would\". So although it was not perfect, it was sufficient.\n",
        "- History: When I continue to produce more and more interations of generated text, the generator on the more complex model seemed to learn as the outputs became more and more what I was looking for. So the first iterations may have been lackluster, but after generating multiple times the outputs became closer and closer to what I wanted, seemingly learning from the past."
      ],
      "metadata": {
        "id": "6KghPD38EEo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i7lbXtiULbOe"
      }
    }
  ]
}